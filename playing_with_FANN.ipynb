{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy.misc\nimport matplotlib.pyplot as plt\nfrom random import randint\nfrom scipy import misc\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "158fc5a6-6259-4589-98a9-4401b5893f17",
        "_uuid": "eff62c2f56e66f746e02b0ad219cc258da2864ac",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Haohan's preprocessing functions\n\ndef threshold_(img, thres=250):\n    img[img > thres] = 255\n    img[img <= thres] = 0\n    return img\n\n\ndef update_bound(i, j, bound):\n    if i < bound[0]:\n        bound[0] = i\n    if i > bound[1]:\n        bound[1] = i\n    if j < bound[2]:\n        bound[2] = j\n    if j > bound[3]:\n        bound[3] = j\n    return bound\n\n\ndef find(x, i, j, record, bound=None):\n    # s_h, e_h, s_w, e_w = 0, 0, 0, 0\n    if i < 0 or i > 63 or j < 0 or j > 63:\n        return bound\n\n    if record[i][j] > 0.5 or x[i][j] < 240:\n        return bound\n\n    record[i][j] = 1\n\n    if bound is None:\n        bound = [0, 0, 0, 0]\n\n    bound = update_bound(i, j, bound)\n\n    step = [[1, 0], [0, -1], [-1, 0], [0, 1]]\n    for step_ in step:\n        find(x, i + step_[0], j + step_[1], record, bound)\n\n    return bound\n\n\ndef detect(x):\n    record = np.zeros(x.shape, dtype=np.int32)\n    max_area = 0\n    max_bound = None\n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n            if record[i][j] > 0.5 or x[i][j] < 240:\n                continue\n            bound = [i, i, j, j]\n            bound = find(x, i, j, record, bound)\n            if bound is not None:\n                # cur_area = (bound[1] - bound[0]) * (bound[3] - bound[2])\n                cur_area = max(bound[1] - bound[0], bound[3] - bound[2])\n                if cur_area > max_area:\n                    max_area = cur_area\n                    max_bound = bound\n\n    # print bound\n    padding = 2\n\n    # padding the bound\n    max_bound[0] = max_bound[0] - padding if max_bound[0] - padding > 0 else 0\n    max_bound[1] = max_bound[1] + padding if max_bound[1] + padding < 64 else 64\n    max_bound[2] = max_bound[2] - padding if max_bound[2] - padding > 0 else 0\n    max_bound[3] = max_bound[3] + padding if max_bound[3] + padding < 64 else 64\n\n    return x[max_bound[0]: max_bound[1], max_bound[2]: max_bound[3]]\n\n\ndef padding(image):\n    img_size = (32, 32)\n    h, w = image.shape\n    pad_h = int((max(h, w) - h) / 2)\n    pad_w = int((max(h, w) - w) / 2)\n    new_image = np.zeros([h + 2 * pad_h, w + 2 * pad_w], dtype=np.float32)\n    new_image[pad_h: pad_h + h, pad_w: pad_w + w] = image\n    new_image = misc.imresize(new_image, img_size)\n    return new_image\n\ndef preprocess(image): \n    return threshold_(padding(detect(image)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Haohan's model\n\n# utilities \n\ndef softmax(output_array):\n    logits_exp = np.exp(output_array)\n    return logits_exp / np.sum(logits_exp, axis=1, keepdims=True)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    # derivative of the sigmoid function\n    return -1 * np.exp(-x) / ((1+np.exp(-x)) ** 2)\n\ndef categorical_crossentropy(output, target):\n    assert output.shape[1] == target.shape[1], \"model output\" + str(output.shape) + \\\n                                               \" should have same shape with target\" + str(target.shape)\n    # output /= np.sum(output, axis=len(output.shape) - 1, keepdims=True)\n    output = softmax(output)\n    # manual computation of crossentropy\n    epsilon = np.finfo(float).eps\n    output = np.clip(output, epsilon, 1. - epsilon)\n    return np.mean(- np.sum(target * np.log(output), axis=len(output.shape) - 1)), target - output\n\n\n# layer classes\n\nclass Layer:\n    def __init__(self):\n        pass\n    def forward(self, b_input):\n        pass\n    def backward(self, loss, lr):\n        pass\n\nclass Sigmoid(Layer):\n    def __init__(self):\n        Layer.__init__(self)\n        \n    def forward(self, b_input):\n        return sigmoid(b_input)\n    \n    def backward(self, loss, lr):\n        return loss * sigmoid_derivative(loss)\n\nclass FC(Layer):\n    def __init__(self, units):\n        Layer.__init__(self)\n        self.w = None\n        self.b = np.random.randn(1, units)\n        self.output = 0\n        self.b_input = None\n        self.units = units\n\n    def forward(self, b_input):\n        self.b_input = b_input\n        if self.w is None:\n            self.w = np.random.randn(self.b_input.shape[1], self.units)\n        z1 = self.b_input.dot(self.w) + self.b\n        # self.output = sigmoid(z1)\n        return z1\n\n    def backward(self, loss, lr=0.001, reg_lambda=0.0001):\n        dw = self.b_input.T.dot(loss)\n        dw += reg_lambda * self.w  # add penalty\n\n        db = np.sum(loss, axis=0, keepdims=True)\n\n        top_loss = loss.dot(self.w.T)\n\n        self.w += lr * dw\n        self.b += lr * db\n\n        return top_loss\n    \n    \n# main model class\n\nclass Fann_Model:\n    def __init__(self, input_shape):\n        self.layers = []\n        self.outputs = []\n        self.gradient = []\n        self.lr = 0.001\n        self.input = None\n\n    def add(self, layer):\n        self.layers.append(layer)\n        self.outputs.append([])\n        self.gradient.append([])\n\n    def forward_all(self):\n        for i, layer in enumerate(self.layers):\n            if i == 0:\n                self.outputs[i] = layer.forward(self.input)\n                continue\n            self.outputs[i] = layer.forward(self.outputs[i - 1])\n\n    def back_propagation(self):\n        for i, layer in reversed(list(enumerate(self.layers))):\n            self.gradient[i] = layer.backward(self.gradient[i + 1], self.lr)\n\n    def get_accuracy(self, output, target):\n        # print output\n        # print target\n        _ = np.array([i.argmax() for i in output]).T\n        y = np.array([i.argmax() for i in target]).T\n\n        total = len(output)\n        right = (_ == y).sum()\n\n        return float(right) / float(total)\n\n    def fit(self, x, y, max_epoch=5, lr=0.0001):\n        self.input = x\n        self.lr = lr\n        if len(self.gradient) == len(self.outputs):\n            self.gradient.append([])\n            #print(len(self.outputs), len(self.gradient))\n\n        assert len(x.shape) == 2, \"X should be in 2 dimensions like (40, 2)\"\n        assert len(y.shape) == 2, \"y should be in 2 dimensions like (40, 1)\"\n\n        losses = []\n        for epoch in range(max_epoch):\n            # forward\n            self.forward_all()\n            loss, gradient = categorical_crossentropy(self.outputs[-1], y)\n\n            accuracy = self.get_accuracy(self.outputs[-1], y)\n            #print(\"Current epoch:\", epoch, \"Loss:\", loss, \"accuracy:\", accuracy)\n            losses.append(loss)\n\n            self.gradient[-1] = gradient\n\n            # backward\n            self.back_propagation()\n\n        return losses\n\n    def predict(self, x):\n        assert len(x.shape) == 2, \"X should be in 2 dimensions like (40, 2)\"\n        self.input = x\n        self.forward_all()\n        return self.outputs[-1]\n\n    def evaluate(self, x, y):\n        pred = self.predict(x)\n        accuracy = self.get_accuracy(pred, y)\n        return accuracy",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "03b1b677-a26f-4a3b-81b3-144e262a86df",
        "_uuid": "8769e574a5cd8fa1c8031f4542f52940c64b17a0",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# make training and validation sets\n# k is the number of sets\n\ndef crossValidation(X, y, k=10): \n    X_train = []\n    X_valid = []\n    y_train = []\n    y_valid = []\n\n    fold_length = int (len(X) / k)\n    for i in range(k):\n        fold_start = i*fold_length\n        fold_end = fold_start + fold_length\n        X_valid.append(X[fold_start:fold_end])\n        X_train.append(np.concatenate((X[:fold_start], X[fold_end:])))\n        y_valid.append(y[fold_start:fold_end])\n        y_train.append(np.concatenate((y[:fold_start], y[fold_end:])))\n        \n    return X_train, X_valid, y_train, y_valid",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4d02a06f-5971-4b41-ab10-37ad8e4bb023",
        "_uuid": "5b2e423535fd931053774bddd2b2326163ab33ef",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create the model and build the layers\n# the hyperparameters are: num_folds, num_layers, num_neurons, num_epochs, learning_rate\n\ndef runModel(X, y, num_folds, num_layers=2, num_neurons=10, num_epochs=100, learning_rate=0.001): \n    \n    X_train, X_valid, y_train, y_valid = crossValidation(X, y, k=num_folds)\n    \n    model = Fann_Model(X.shape)\n    for i in range(num_layers - 1): \n        model.add(FC(num_neurons))\n        model.add(Sigmoid())\n    # the last layer has as many units as there are classes\n    model.add(FC(classes))\n    model.add(Sigmoid())\n    \n    # train the model\n    validation_scores = []\n    for i in range(num_folds): \n        #print(\"\\nTraining with fold\", i)\n        model.fit(X_train[i], y_train[i], max_epoch=num_epochs, lr=learning_rate)\n        score = model.evaluate(X_valid[i], y_valid[i])\n        #print(\"Validation score for fold \", i, \":\", score)\n        validation_scores.append(score)\n\n    mean_score = np.mean(validation_scores)\n    #print(\"\\nAverage validation score:\", mean_score)\n    return mean_score\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9d6bb34b-953b-4a9a-af18-556a14e0921f",
        "_uuid": "2d31c0912c6cf4fde3ff4db7890f829d73659de1",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# input data\n\nX_total = np.loadtxt(\"../input/train_x.csv\", delimiter=\",\") # load from text \ny_total = np.loadtxt(\"../input/train_y.csv\", delimiter=\",\") \nX_total = X_total.reshape(-1, 64, 64) # reshape \ny_total = y_total.reshape(-1, 1) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9aa2a26f-a1be-480f-a5f6-77b018df5776",
        "_uuid": "fbd01caaa0b792c905810c5753673ac316706e22",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# preparation\n\n# here I get smaller subsets for testing\nX = X_total[:300]\ny = y_total[:300]\n\n# 1-of-10 categorization for the classes\nfrom keras.utils import to_categorical\nclasses = 10\ny = to_categorical(y, num_classes=classes)\n\n# image preprocessing \nX = np.array([preprocess(x) for x in X])\n\n# reshaping the input array in 2D\n#print(X.shape)\nX = X.reshape(X.shape[0], -1)\n#print(X.shape)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c0097e49-99f8-4bb0-ab3e-35a71a7c4938",
        "_uuid": "e463878fbe28820b9a864975b2f242b60a9194b2",
        "collapsed": true,
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Running the model\n\n# hyperparameters\nfolds = 10\nlayers = 2  #number of layers in the network\nneurons = 10 #number of units per layer (except the last layer)\nepochs = 10  #number of epochs for training\nlr = 0.001 #learning rate\n\n#validation_result = runModel(X, y, num_folds=folds, num_layers=layers, num_neurons=neurons, num_epochs=epochs, learning_rate=lr)\n#print(\"\\nAverage validation score:\", validation_result)\n\n# Loop with several values of hyperparameters to find the best combination.\n# WARNING: do not put ranges that are too large, otherwise this will take forever. \n# It's better to fix most hyperparameters and vary only one of them. \n# An asterisk in the output indicates the best validation score so far in the current run.\nbest_result = 0\nfor layers in range(2, 6):\n    for neurons in range(2, 3): \n        for epochs in range(50, 60, 10):\n            for lr in np.logspace(-5, -2, num=4): \n                validation_result = runModel(X, y, num_folds=folds, num_layers=layers, num_neurons=neurons, num_epochs=epochs, learning_rate=lr)\n                asterisk = \"\"\n                if validation_result > best_result: \n                    best_result = validation_result\n                    asterisk = \"*\"\n                print(\"%sAverage validation score with folds=%s, num_layers=%s, num_neurons=%s, num_epochs=%s, learning_rate=%s: %s\" % (asterisk, folds, layers, neurons, epochs, lr, validation_result))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9158be7b-7b41-4929-9269-bdb96237b948",
        "_uuid": "b3328abca2507c4862047714f8af892d6ae378c5",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}